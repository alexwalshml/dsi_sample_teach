{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2894a8f9-863d-467a-98a9-e2f685fb4686",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are a *tree based* method that can be used for regression or classification. Decision trees are a non-parametric model, meaning that few underlying assumptions about the data have to made. DTs are a simple, light-weight model, and are often used as a unit component of more complex models.\n",
    "\n",
    "## Tree Structure\n",
    "\n",
    "A [tree](https://en.wikipedia.org/wiki/Tree_(data_structure)) is a type of data structure consisting of a set of nodes. If a node has no parents (nodes above it in the tree structure), it is considered to be a *root node*. Similarly, if the node has no children (nodes below it), it is considered a *leaf node*. All nodes with exactly one parent and at least one child are called *decision nodes* (or, more generally, an *inner node*).\n",
    "\n",
    "<img src=\"assets/decision_tree.png\" align=\"center\"/>\n",
    "\n",
    "[Image source](https://www.smartdraw.com/decision-tree/)\n",
    "\n",
    "Decision trees are a special type of *binary tree*, in which all decision nodes have exactly two children. These nodes represent decisions, or ways to split the data into two subsets. Each node asks a binary question about the data, and all points for which that question is true go into one child node, and the rest go into the other child node. This process repeats until the splitting criterion determines that all points in all nodes should no longer be split, completing the tree.\n",
    "\n",
    "## Regression Trees\n",
    "\n",
    "In regression problems you have seen in the past, you may have noticed that the data behaves differently in different parts of the domain. If using any sort of standard regression, this would be a source of bias in the model. Things fundamentally change depending on the value of $\\overrightarrow{x}$, and thus a single line is not suitable for prediction. Consider the following simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4754d8-47ef-4d1f-a27a-e3109ffce3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2f6e8-637b-4de1-bd30-7d044460cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array to represent a dataset\n",
    "x = np.linspace(0, 10, 1000).reshape(-1, 1)\n",
    "\n",
    "# create our label y\n",
    "y = x.copy()\n",
    "# and then shift half of it up by ten\n",
    "y[500:] += 10\n",
    "\n",
    "# fit with ols linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "ols = lr.predict(x)\n",
    "\n",
    "# and plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(x, y, color=\"blue\", label=\"y\", s=20)\n",
    "ax.scatter(x, ols, color=\"orange\", label=\"OLS\", s=3)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"OLS Fit of a Non-Continuous Set\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55caf6f-bddd-4c9b-9d14-57169ed0cc89",
   "metadata": {},
   "source": [
    "In this example, our data clearly follows two distinct linear trends. However, the ordinary least squares algorithm doesn't know how to look for this. It fits on *all* of the data, and threfore is only accurate at two point. This is an easy fix, and all we have to do to get a perfect fit is to create two OLS fits instead of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151d84b-2d93-429c-9191-e45ea6acf4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two distinct sets for fitting\n",
    "x1 = x[:500]\n",
    "x2 = x[500:]\n",
    "\n",
    "# the target as well\n",
    "y1 = x1.copy()\n",
    "y2 = x2 + 10\n",
    "\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(x1, y1)\n",
    "ols1 = lr1.predict(x1)\n",
    "\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(x2, y2)\n",
    "ols2 = lr2.predict(x2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(x, y, color=\"blue\", label=\"y\", s=20)\n",
    "ax.scatter(x1, ols1, color=\"orange\", label=\"OLS\", s=3)\n",
    "ax.scatter(x2, ols2, color=\"orange\", s=3)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Split OLS Fit of a Non-Continuous Set\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905db7d-04b5-4cba-a149-85d05eb5fdd1",
   "metadata": {},
   "source": [
    "However, this leaves quite a bit to be desired. What if the split isn't especially visable, or what if there is more than one? How would you handle two, three, ten, or one thousand splits? Manually coding indivdual OLS fits just isn't an option (not without a method to automate changes in the data, at least). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d04ac7-e293-4040-bded-f77c26852135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable that determines how many times\n",
    "# a random subset of the data will be shifted up or down\n",
    "NUM_SHIFTS = 10\n",
    "DATA_SIZE = 10000\n",
    "\n",
    "x = np.linspace(0, 10, DATA_SIZE).reshape(-1, 1)\n",
    "y = x.copy()\n",
    "\n",
    "for shift in range(NUM_SHIFTS):\n",
    "    # choose a random index in the data as the lower bound\n",
    "    bound1 = np.random.randint(low=0, high=DATA_SIZE)\n",
    "    # choose a second random index as the upper bound\n",
    "    bound2 = np.random.randint(low=bound1, high=DATA_SIZE)\n",
    "    # generate a random number to shift the data by\n",
    "    r = 3 * np.random.random()\n",
    "    # add the random number to all y values between the bounds\n",
    "    y[bound1: bound2] += r\n",
    "\n",
    "# plot the now chaotic data\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(x, y, color=\"blue\", label=\"y\", s=20)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Randomly Shifted Linear Data\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe3ba1-2d54-4a36-82ff-068c8f0088a0",
   "metadata": {},
   "source": [
    "Regression trees handle this exact problem. Implement a DecisionTreeRegressor object and fit it to our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b7cc0-581d-4347-a192-709d458ac2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement and fit a dt object\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(x, y)\n",
    "\n",
    "# predict y values\n",
    "dtr_pred = dtr.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(x, y, color=\"blue\", label=\"y\", s=20)\n",
    "ax.scatter(x, dtr_pred, color=\"orange\", label=\"DTR\", s=3)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"DTR Fit of Randomly Shifted Linear Data\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cfcc7-26e6-4093-abde-83a02a413854",
   "metadata": {},
   "source": [
    "The DT fits the data *perfectly*. This is expected, as the data we created was perfectly linear on each subdomain. While there is some real world data that is segmented like this, the reality is that most of the time data is fairly continuous, and in those cases a DT will just be a set of very localized approximations.\n",
    "\n",
    "### How to split the data?\n",
    "\n",
    "Just as linear regression aims to minimize the error of its predictions, so does a regression tree. The default quanitity to minimize is the squared error, but other quantities such as the mean absolute error are available as well. Historically, minimization of statistical quantities such as variance or chi-squared was used as well, though these quantities are less interpretable.\n",
    "\n",
    "### (Extra) Splitting Condition: Variance\n",
    "\n",
    "Write a function that accepts a set of one-dimensional data and chooses a point to split on that minimizes the total variance of the two child sets, then returns those two sets. How would you modify your function to work with multi-dimensional data? How would you implement a function that minimizes MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd28b44-30d7-4011-8d6e-1eeb2a9be5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(x):\n",
    "    variance = []\n",
    "    for n in range(len(x)):\n",
    "        c1, c2 = x[:n], x[n:]\n",
    "        var = np.var(c1) + np.var(c2)\n",
    "        variance.append(var)\n",
    "    \n",
    "    split = np.argmin(variance)\n",
    "    \n",
    "    return x[:argmin], x[argmin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f8556-20e2-4195-a735-a63d32f62940",
   "metadata": {},
   "source": [
    "### What about regularization?\n",
    "\n",
    "A quick read of the [decision tree regressor documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) shows that the MSE used in splitting is just the $L^2$ loss with no Lasso or Ridge term. Similarly, there is no option to specify which type of regression is used in the fits. DTs are meant to be low-bias models, but letting them go unchecked would be unwise. How would you prevent a decision tree from over fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82072b82-c65b-42a9-9d67-81e3645572c6",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "* Restrict depth\n",
    "* Restrict number of leaf nodes\n",
    "* Restrict number of samples per node\n",
    "* Increase `ccp_alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed582a-754c-4bde-8015-fb83c8defdff",
   "metadata": {},
   "source": [
    "### The Regression Tree Algorithm\n",
    "\n",
    "To build a decision tree, the algorithm must repeat the same steps at each node:\n",
    "\n",
    "1. Loop through each feature\n",
    "\n",
    "2. For each feature, compute the total MSE (or whatever splitting qunatity was specified) of the two splits for every single possible splitting point\n",
    "\n",
    "3. Choose the feature-value pair which minimizes MSE\n",
    "\n",
    "This process is repeated until any number of stopping criterion are met. Common stopping criterion are a max depth for the tree, a minimum number of samples per leaf, or the tree not being able to optimize any further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52e1e6-23f1-4320-8a95-ea38d09c513d",
   "metadata": {},
   "source": [
    "### Check your understanding\n",
    "\n",
    "* Are regression trees a low or high bias model? How about variance?\n",
    "\n",
    "* When should one use regression trees?\n",
    "\n",
    "* Are there any shortcomings of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf284a60-74f0-4891-88fe-5e88bd1fed19",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "\n",
    "Similar to the regression case, decision trees can also be used to classify. In the very same fashion, the DT will ask a series of \"yes or no\" questions about the data to try and make it so each leaf node contains only a single category. For example, consider a set of questions that you might ask to determine if an animal is a dog.\n",
    "\n",
    "You might first ask if it has four legs. This would eliminate primates and birds, as well as most bugs from child set containing dogs. Then, on the set of the data that did have four legs, you might ask if the creature has fur or not to eliminate reptiles and amphibians from the data. Then, on the set which has fur, you may ask if the weight of the animal is above 15 pounds, but this would produce two sets which both contain dogs: big dogs, and small dogs. For the small dogs, you could ask if the animal has retractable claws to remove cats from the set, while on the large dogs you may choose to split on weight again to remove large mammals from the set.\n",
    "\n",
    "<img src=\"assets/animal_tree.png\" align=\"center\"/>\n",
    "\n",
    "As the initial set of animals gets larger, one would have to ask more and more questions to accurately categorize each animal. For example, if a rabbit were put through the above set of questions, it would follow the same path as the small dog, and thus would be classified incorrectly.\n",
    "\n",
    "### How to split the data?\n",
    "\n",
    "If you were to try to quantify the decision making process, how would you do so? An intuitive measure is to try to make each child set as pure as possible. If a child set contains only one category, then it is perfectly pure and nothing more needs to be done with that data. We implement this logic using a quantity called the *Gini impurity*,\n",
    "\n",
    "$$\n",
    "G = \\sum_{c} P(c|t)(1 - P(c|t)) = 1 - \\sum_{c} P^{2}(c|t).\n",
    "$$\n",
    "\n",
    "Here, $P(c|t)$ represents the probability of observing category $c$ at node $t$. Then, each term in the first version of the sum can be interpreted as the probability that category $c$ is guessed ($P(c|t)$) *and* is incorrect ($1 - P(c|t)$). Therefore, when summed over, the Gini impurity represents the probability guessing the wrong category. Minimizing this quantity will maximize the number of correctly labeled outputs, and that is exactly what decision trees do. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd80a7-23ef-4809-bf76-a3828bd0dc44",
   "metadata": {},
   "source": [
    "### (Extra) Splitting condition: Shannon Entropy\n",
    "\n",
    "Another common splitting condition for classifier trees is to minimize the *entropy* of the two child sets. Entropy is fundamentally an information quantity, and the higher the entropy of a data set, the less about it we know. The Shannon entropy is defined as\n",
    "\n",
    "$$\n",
    "H(x) = \\sum_{i} -P(x_i) \\log(P(x_i)) = \\sum_{i} P(x_i) \\log\\left(\\frac{1}{P(x_i)}\\right).\n",
    "$$\n",
    "\n",
    "This formula is far from intuitive, so here are a few different ways to interpret entropy:\n",
    "\n",
    "* Entropy is a measure of the information in a system that we *cannot* know\n",
    "\n",
    "* The binary ($\\log_2$) entropy is a measure of the smallest number of bits that could theoretically represent a binary message\n",
    "\n",
    "* Entropy measures how many different *microstates* give rise to the same *macrostate*\n",
    "\n",
    "Chances are, none of those clear up the above formula, yet they are true. Why? Let's examine the logarithmic term, first. More specifically, we need intuition for the inverse probability. Imagine a situation where a coin is flipped, and you are asked to ask a single yes-or-no question to determine the result. For example, \"was it heads?\", and, regardless of the answer, you now know the outcome of the coin flip. By asking *one* binary question, you reveal which of *two* uncertain outcomes with probability $1/2$ occured. This corresponds to an inverse probability $1/\\frac{1}{2}=2$. Let's take things a step further.\n",
    "\n",
    "Imagine the same situation, but replace the coin with a die, and you are asked to guess the face on which it lands. If you guess wrong, the only thing you know is that it didn't land on your specified face. But if you guess right, you know that it *didn't land on any other face*. A positive event on the face you monitored tells the outcome of six possible events. $1/\\frac{1}{6}=6$. Starting to see the pattern? The less likely an event is, the more it tells you about the system when that event actually does occur.\n",
    "\n",
    "So why is it logarithmic? Logarithms added together is the same as numbers multiplying. Since probabilities combine by multiplying, we can reach the same result by adding their logarithms in a way that plays nice with differential calculus.\n",
    "\n",
    "The last remaining piece is the probability that gets multiplied onto each logarithm. To understand this, consider the dice example again, but this time the dice will have painted faces. The \"one\" face will be painted green, and the other five faces will be painted red. Further more, the paint obscures the number so tht it cannot be determined. Now roll the dice. What number did you roll? If the green face was up, you know exactly: you rolled a one, and you didn't roll a two through six. This event has high information. But what if you rolled a red? In that case, the only thing you know is that you *didn't roll a one*. There are still five uncertain states in play. Thus, this event is low information. The high-information event is useful, but is unlikely, whereas the low-information events are almost useless, but will happen frequently. Both contribute to our knowledge about the system, and thus should be added together. Therefore, the total information in every possible outcome is the information of that outcome ($-\\log P_i$) weighted by the probability of that outcome.\n",
    "\n",
    "Therefore, by minimizing the entropy, we are maximizing the information we have about the data. This is analagous to minimizing purity, because a pure set has zero entropy.\n",
    "\n",
    "### Back to the data\n",
    "\n",
    "To prove the effectiveness of classification trees on real data, we'll use the familiar Titanic dataset. Since this is already familiar data, there's no need to spend much time cleaning and encoding the data. We'll simply drop the null values, as well as any columns which aren't immediately processable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a04aa8-6405-49aa-a597-cf600b8c2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data, and drop any columns with null values\n",
    "titanic_data = pd.read_csv('data/titanic.csv')\n",
    "titanic_data = titanic_data.dropna(axis=1)\n",
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b21f6d-3d88-4978-92ac-5435ec126c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the sex column\n",
    "# then drop it, along with all other non-numeric columns (and Id)\n",
    "sexes = pd.get_dummies(titanic_data[\"Sex\"], drop_first=True)\n",
    "titanic_data = titanic_data.drop(columns=[\"Name\", \"Ticket\", \"PassengerId\", \"Sex\"])\n",
    "titanic_data = pd.concat([titanic_data, sexes], join=\"inner\", axis=1)\n",
    "titanic_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fcb9b-ab2c-4fad-aa4d-9b06bad9a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our train and test splits\n",
    "target = \"Survived\"\n",
    "features = [col for col in titanic_data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_data[features], titanic_data[target], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05b94e-fc24-4ef0-a15c-4cdf58aeb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an instance of DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f0a21-e607-4bf1-85ee-35915ab1d63e",
   "metadata": {},
   "source": [
    "Now, let's see how our model did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e075a-a6e8-4ff1-885d-09ac4843ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training score: {dtc.score(X_train, y_train)}\\nTest score: {dtc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c42ec-bab0-4dd1-9bfc-74f3cb8dfe31",
   "metadata": {},
   "source": [
    "Looks like our model is fairly over fit. In general, without any limitations, a decision tree will keep splitting the data until the impurity can't be lowered any more, even if this means that each leaf only contains one entry. Let's see the total depth (read: longest chain of nodes) of our default classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739be04-bbad-404a-8fb6-dfcf233d752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8cae3-5d78-4948-bbc6-e73794d6c72f",
   "metadata": {},
   "source": [
    "To help avoid over fitting, we can implement a value for `max_depth`, which will restrict how many nodes our model can contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c599b-566b-483d-a55a-00368a3dcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc3 = DecisionTreeClassifier(max_depth=3)\n",
    "dtc3.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e15189-73a8-40ac-aa97-5128f0898779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training score: {dtc3.score(X_train, y_train)}\\nTest score: {dtc3.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cef397-e381-4404-a375-3d6b23956ead",
   "metadata": {},
   "source": [
    "This model has much higher bias than before, however, this time it has nearly equal performance on new data.\n",
    "\n",
    "### Viewing the tree structure\n",
    "\n",
    "sklearn makes it easy to view the structure of a decision tree. Using the `plot_tree` function, we can view the nodes and what is happening at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15fb6f-cd9e-4d35-a932-884a48f330c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "plot_tree(dtc3, feature_names=features, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a79b5-df6b-4603-a2da-3671235a199a",
   "metadata": {},
   "source": [
    "In addition, this allows us to infer information about our data in a hierarchal sense. The very first feature the model chose to split on was whether the passenger was male or female. This makes quite a bit of sense, given that the Titanic evacuation prioritized women and children. The next thing it prioritized was fare and class, both of which hold essentially the same information, which again makes sense. The rich passengers had much better survival rates compared to the poor.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Decision trees also have a way to quantify the importance of each feature. The more the impurity is reduced by splitting the data on a feature, the more important it is. However, deep down in the tree, relatively pure data might be entering the lower nodes, so it doesn't make sense to give equal importance to the lower nodes. The importance is therefore calculated as the total impurity reduction at each node weighted by the probability of reaching that node. This gives the most weight to the root node, and the least weight to the deepest nodes in the tree. The DTC object has an importance attribute which will automatically calculate this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ce081-9b4b-4014-a434-4342c0b3590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dtc3.feature_importances_\n",
    "\n",
    "for feature, imp in zip(features, importances):\n",
    "    print(f\"The Gini importance of {feature} was {imp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b792e-c2dc-420b-8ac0-ed4086529fd2",
   "metadata": {},
   "source": [
    "### Check your understanding\n",
    "\n",
    "* Which splitting metric would you use to explain the DT process to a client? Why?\n",
    "\n",
    "* Would DTs generalize well to new data?\n",
    "\n",
    "* What does the tree structure tell us about the decision making process?\n",
    "\n",
    "## Decision Trees: Pros and Cons\n",
    "\n",
    "### The good\n",
    "\n",
    "* DTs are lightweight models\n",
    "* Non-parametric\n",
    "* Low bias\n",
    "* Intepretable\n",
    "* Great for inference\n",
    "\n",
    "### The bad\n",
    "\n",
    "* High variance\n",
    "* Can't compete with complex models\n",
    "* Need lots of data to properly fit\n",
    "* Need relatively balanced classes\n",
    "\n",
    "### The ugly\n",
    "\n",
    "DTs are rarely used on there own. If a simple/interpretable model is needed, there are other options which can often perform better. However, they are exellent when used in groups as part of an ensemble model, such as random forests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
